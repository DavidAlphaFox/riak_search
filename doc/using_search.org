#+SETUPFILE: "basho-search-doc-style.iorg"
#+TITLE: Riak Search 0.12 Manual
#+AUTHOR: Basho Technologies
#+DATE: 20 July 2010

* Introduction

  Basho Riak Search is a distributed index and full-text search engine
  built on top of (and complementary to) Basho Riak. Just like Riak,
  Search allows you to store your data in a distributed fashion
  designed for easy operations. You can add physical servers to gain
  capacity and performance, and if a node fails the cluster continues
  to function.

** Requirements

   The following components are required:

   + Erlang R13B04
   + Java 1.6.x
   + Ant
   + gcc toolchain
   + BDB Java Edition 4.0.103 - (Installation instructions below.)

** Installation

*** Unzip and install Riak Search*

    Enter the riak\_search directory and run:

    : make
    : make rel

    Change to the rel/riaksearch directory

    : cd rel/riaksearch

*** Start Riak

    If you are running on a Mac, set JAVA\_HOME and increase your
    filehandle limit:

    : export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home
    : ulimit -n 1024

    From within the rel/riaksearch directory, start Riak:

    : bin/riaksearch console

    Or, use =bin/riaksearch start= to start riak\_search in the background.


** Major Components

   Riak Search is comprised of:

   + *Riak Core* -  Dynamo-inspired distributed-systems framework
   + *Riak KV* - Distributed Key/Value store inspired by Amazon's Dynamo.
   + *Bitcask* -  Storage backend used by Riak KV.
   + *Riak Search* - Distributed index and full-text search engine.
   + *MergeIndex* - Storage backend used by Riak Search.
   + *Qilr* - Library for parsing queries into execution plans and
             documents into terms.
   + *Riak Solr* - Adds a subset of Solr HTTP interface capabilities to
                  Riak Search.

   Note that even though Riak KV is included in Riak Search, it
   *SHOULD NOT* be used to store application data. It is used intenally
   by Search to store document data, schema definitions, and other
   configuration data. Documents are stored in a bucket with the same
   name as the search index, where the key is the value of the
   document's ID field.

** Replication

   Search data is replicated in a manner similar to Riak KV
   data: A search index has an =n_val= setting that determines how
   many copies of the data exist. Copies are written across different
   partitions located on different physical nodes.

   In contrast to Riak KV:

   + Search uses timestamps, rather than vector clocks, to resolve
     version conflicts. This leads to fewer guarantees about your data
     (as depending on wall-clock time can cause problems if the clock is
     wrong) but was a necessary tradeoff for performance reasons.

   + Search does not use quorum values when writing (indexing)
     data. The data is written in a fire and forget model. Search
     *does* use hinted-handoff to remain write-available when a node
     goes offline.

   + Search does not use quorum values when reading (querying)
     data. Only one copy of the data is read, and the partition is
     chosen based on what will create the most efficient query plan
     overall.

* Schema

  Riak Search was designed to work seamlessly with Riak. As a result,
  it retains many of the same properties as Riak, including a
  schema-free design. In other words, you can start adding data to a
  new index without having to explicitly define the index fields.

  That said, Search does provide the ability to define a custom
  schema. This allows you to specify required fields and custom
  analyzer factories, among other things.

** The Default Schema

   The default schema treats all fields as strings, unless you suffix
   your field name as follows:

   + *FIELDNAME\_num* - Numeric field. Uses Whitespace analyzer.

   + *FIELDNAME\_dt* - Date field. Uses Whitespace analyzer. 

   + All other fields are treated as Strings and use the Standard analyzer.

   The default field is named *value*.

** Defining a Schema

   The schema definition for an index is stored in the Riak
   bucket of the same name as the index, under the key
   =_rs_schema=. For example, the schema for the "books" index is
   stored under =books/_rs_schema=.

   Alternatively, you can set or retrieve the schema for an index
   using command line tools:

   : # Set an index schema.
   : bin/search-cmd set_schema Index SchemaFile
   :
   : # View the schema for an Index.
   : bin/search-cmd show_schema Index

   Note that changes to the Schema File *will not* affect previously
   indexed data. It is recommended that if you change field
   definitions, especially settings such as /type/, that
   you re-index your documents.

   Below is an example schema file. The schema is formatted as an
   Erlang term. Spacing does not matter, but it is important to
   matching opening and closing brackets and braces, to include
   commas between all list items, and to include the final period
   after the last brace:

   : {
   :     schema,
   :     [
   :         {version, "1.1"},
   :         {default_field, "value"},
   :         {default_op, "or"},
   :         {analyzer_factory, "com.basho.search.analysis.DefaultAnalyzerFactory"}
   :     ],
   :     [
   :         {field, [
   :             {name, "id"},
   :             {type, string}
   :         ]},
   :         {field, [
   :             {name, "title"},
   :             {required, true},
   :             {type, string}
   :         ]},
   :         {field, [
   :             {name, "published"},
   :             {type, date}
   :         ]},
   :         {dynamic_field, [
   :             {name, "*_text"},
   :             {type, string}
   :         ]},
   :         {field, [
   :             {name, "tags"},
   :             {type, string},
   :             {analyzer_factory, "com.basho.search.analysis.WhitespaceAnalyzerFactory"}
   :         ]},
   :         {field, [
   :             {name, "count"},
   :             {type, integer},
   :             {padding_size, 10}
   :         ]},
   :         {field, [
   :             {name, "category"}
   :         ]}
   :     ]
   : }.

*** Schema-level properties:

    The following properties are defined at a schema level:

    + *version* - Required. A version number, currently unused.

    + *default\_field* - Required. Specify the default field used for
      searching.

    + *default\_op* - Optional. Set to "and" or "or" to define the
      default boolean. Defaults to "or".

    + *analyzer\_factory* - Optional. Defaults to
      "com.basho.search.analysis.DefaultAnalyzerFactory"

*** Fields and Field-Level Properties:

    Fields can either by static or dynamic. A static field is denoted
    with 'field' at the start of the field definition, whereas a
    dynamic field is denoted with 'dynamic\_field' at the start of the
    field definition.

    The difference is that a static field will perform an exact string
    match on a field name, and a dynamic field will perform a wildcard
    match on the string name. The wildcard can appear anywhere within
    the field, but it usually occurs at the beginning or end. (The
    default schema, described above, uses dynamic fields, allowing you
    to use fieldname suffixes to create fields of different data
    types.)

    Field matching occurs in the order of appearance in the schema
    definition. This allows you to create a number of static fields
    followed by a dynamic field as a "catch all" to match the rest.

    The following properties are defined at a field level, and apply
    to both static and dynamic fields:

    + *name* - Required. The name of the field. Dynamic fields can use
      wildcards. Note that the unique field identifying a document
      /must/ be named "id".

    + *required* - Optional. Boolean flag indicating whether this
      field is required in an incoming search document. If missing,
      then the document will fail validation. Defaults to false.

    + *type* - Optional. The type of field, either 'string' or
      'integer'. If 'integer' is specified, and no field-level
      analyzer\_factory is defined, then the field will use the
      Whitespace analyzer. Defaults to 'string'.

    + *analyzer\_factory* - Optional. Specify the analyzer factory to
      use when parsing the field. If not specified, defaults to the
      analyzer factory for the schema. (Unless the field is an integer
      type. See above.)

    + *padding\_size* - Optional. Values are padded up to this
      size. Defaults to 0 for string types, 10 for integer types.

* Indexing

  Indexing a document is the act of:

  1. Reading a document.
  2. Splitting the document into one or more fields.
  3. Splitting the fields into one or more terms.
  4. Normalizing the terms in each field.
  5. Writing the ={Field, Term, DocumentID}= postings to an index.

  There are numerous ways to index a document in Riak Search.

** Indexing via the Command Line

   The easiest way to index documents stored on the filesystem is to
   use the =search-cmd= command line tool:

   : bin/search-cmd index <INDEX> <PATH>

   Parameters:

   + *<INDEX>* - The name of an index.

   + *<PATH>* - Relative or absolute path to the files or directories
               to recursively index. Wildcards are permitted.

   For example:

   : bin/search-cmd index my_index files/to/index/*.txt

   The documents will be indexed into the default field defined by the
   Index's schema, using the base filename plus extension as the
   document ID.


** Deleting via the Command Line

   To remove previously indexed files from the command line, use the
   =search-cmd= command line tool:

   : bin/search-cmd delete <INDEX> <PATH>

   Parameters:

   + *<INDEX>* - The name of an index.

   + *<PATH>* - Relative or absolute path to the files or directories
               to recursively index. Wildcards are permitted.

   For example:

   : bin/search-cmd delete my_index files/to/index/*.txt

   Any documents matching the base filename plus extension of the
   files found will be removed from the index. The actual contents of
   the files are ignored during this operation.

** Indexing via the Erlang API

   The following Erlang functions will index documents stored on the
   filesystem:

   : search:index_dir(Path).
   : search:index_dir(Index, Path).

   + *Index* - The name of the index.

   + *Path* - Relative or absolute path to the files or directories to
             recursively index. Wildcards are

   For example:

   : search:index_dir("my_index", "files/to/index/*.txt").

   The documents will be indexed into the default field defined by the
   Index's schema, using the base filename plus extension as the
   document ID.

   Alternatively, you can provide the fields of the document to index:

   : search:index_doc(Index, DocId, Fields)

   Parameters:

   + *Index* - The name of the index.

   + *DocId* - Document Id

   + *Fields* - A Key/Value list of fields to index. One of these
               fields must be either the atom 'id' or the string
               "id".

   For example:

   : search:index_doc(Index, DocId, [{title, "The Title"}, {content, "The Content"}])


** Deleting via the Erlang API

   The following Erlang functions will remove documents from the index:

   : search:delete_dir(Path).
   : search:delete_dir(Index, Path).

   + *Index* - The name of the index.

   + *Path* - Relative or absolute path to the files or directories to
             recursively index. Wildcards are

   For example:

   : search:delete_dir("my_index", "files/to/index/*.txt").

   Any documents matching the base filename plus extension of the
   files found will be removed from the index. The actual contents of
   the files are ignored during this operation.

   Alternatively, you can provide the fields of the document to index:

   : search:delete_doc(Index, DocID)

   Parameters:

   + *Index* - The name of the index.

   + *DocID* - The document ID of the document to delete.


** Indexing via the Solr Interface

   Riak Search supports a Solr-compatible interface for indexing
   documents via HTTP. Documents must be formatted as simple Solr XML
   documents, for example:

   : <add>
   :   <doc>
   :     <field name="id">DocID</field>
   :     <field name="title">Zen and the Art of Motorcycle Maintenance</field>
   :     <field name="author">Robert Pirsig</field>
   :     ...
   :   </doc>
   :   ...
   : </add>

   Additionally, the Content-Type header must be set to 'text/xml'.

   Search currently requires that the field determining the document
   ID be named =id=, and does not support any additional attributes on
   the =add=, =doc=, or =field= elements. (In other words, things like
   =overwrite=, =commitWithin=, and =boost= are not yet supported.)

   The Solr interface does NOT support the =<commit />= nor =<optimize
   />= commands. All data is committed automatically in the following
   stages:

   + Incoming Solr XML document is parsed. If XML is invalid, an error
     is returned.

   + Documents fields are analyzed and broken into terms. If there are
     any problems, an error is returned.

   + Documents terms are indexed in parallel. Their availability in
     future queries is determined by the storage backend.

   By default, the update endpoint is located at
   "http://hostname:8098/solr/update?index=INDEX".

   Alternatively, the index can be included in the URL, for example
   "http://hostname:8098/solr/INDEX/update".

   To add data to the system with Curl:

   : curl -X POST -H text/xml --data-binary @tests/books.xml http://localhost:8098/solr/books/update

   Alternatively, you can index Solr files on the command line:

   : bin/search-cmd solr my_index path/to/solrfile.xml


** Deleting via the Solr Interface

   Documents can also be deleted through the Solr interface via two
   methods, either by Document ID or by Query.

   To delete documents by document ID, post the following XML to the update endpoint:

   : <delete>
   :   <id>docid1</id>
   :   <id>docid2</id>
   :   ...
   : </delete>

   To delete documents by Query, post the following XML to the update endpoint:

   : <delete>
   :   <query>QUERY1</query>
   :   <query>QUERY2</query>
   :   ...
   : </delete>

   Any documents that match the provided queries will be deleted.


* Querying

** Query Syntax

   Riak Search follows the same query syntax as Lucene, detailed
   here:

   http://lucene.apache.org/java/2_4_0/queryparsersyntax.html

*** Terms and Phrases

    A query can be as simple as a single term (ie: "red") or a series
    of terms surrounded by quotes called a phrase ("See spot
    run"). The term (or phrase) is analyzed using the default
    analyzer for the index.

    The index schema contains a =default_operator= setting that
    determines whether a phrase is treated as an AND operation or an
    OR operation. By default, a phrase is treated as an OR
    operation. In other words, a document is returned if it matches
    any one of the terms in the phrase.

*** Fields

    You can specify a field to search by putting it in front of the
    term or phrase to search. For example:

    : color:red

    Or:

    : title:"See spot run"

    You can further specify an index by prefixing the field with the
    index name. For example:

    : products.color:red

    Or:

    : books.title:"See spot run"

*** Wildcard Searches

    Terms can include wildcards in the form of an asterisk (*) to
    allow prefix matching, or a question mark (?) to match a single
    character.

    Currently, the wildcard must come at the end of the term in both
    cases.

    For example:

    + "bus*" will match "busy", "business", "busted", etc.

    + "bus?" will match "busy", "bust", "busk", etc.

*** Fuzzy Searches

    Fuzzy searching allows you to find terms with similar
    spelling. To specify a fuzzy search, use the tilde operator on a
    single term with an optional fuzziness argument. (If no fuzziness
    argument is specified, then 0.5 is used by default.)

    For example:

    : bass~

    Is equivalent to:

    : bass~0.5

    And will match "bass" as well as "bask", "bats", "bars", etc. The
    fuzziness argument is a number between 0.0 and 1.0. Values close
    to 0.0 result in more fuzziness, values close to 1.0 result in
    less fuzziness.

*** Proximity Searches

    Proximity searching allows you to find terms that are within a
    certain number of words from each other. To specify a proximity
    seach, use the tilde argument on a phrase.

    For example:

    : "See spot run"~20

    Will find documents that have the words "see", "spot", and "run"
    all within the same block of 20 words.

*** Range Searches

    Range searches allow you to find documents with terms in between
    a specific range. Ranges are calculated lexicographically.  Use
    square brackets to specify an inclusive range, and curly braces
    to specify an exclusive range.

    The following example will return documents with words containing
    "red" and "rum", plus any words in between.

    : "field:[red TO rum]"

    The following example will return documents with words in between
    "red" and "rum":

    : "field:{red TO rum}"

*** Boosting a Term

    A term (or phrase) can have its score boosted using the caret
    operator along with an integer boost factor.

    In the following example, documents with the term "red" will have
    their score boosted:

    : red^5 OR blue

*** Boolean Operators - AND, OR, NOT

    Queries can use the boolean operators AND, OR, and NOT. The
    boolean operators must be capitalized.

    The following example return documents containing the words "red"
    and "blue" but not "yellow".

    : red AND blue AND NOT yellow

    The required (+) operator can be used in place of "AND", and the
    prohibited (-) operator can be used in place of "AND NOT". For
    example, the query above can be rewritten as:

    : +red +blue -yellow

*** Grouping

    Clauses in a query can be grouped using parentheses. The following
    query returns documents that contain the terms "red" or "blue",
    but not "yellow":

    : (red OR blue) AND NOT yellow

** Querying via the Search Shell

   The Search Shell is the easiest way to run interactive queries
   against Search. To start the shell, run:

   : bin/search-cmd shell [INDEX]

   This launches an interactive console into which you can type search
   commands. For help, type =h()=.


** Querying via the Command Line

   To run a single query from the command line, use:

   : bin/search-cmd search [INDEX] QUERY

   For example:

   : bin/search-cmd search books "title:\"See spot run\""

   This will display a list of Document ID values matching the
   query. To conduct a document search, use the *search\_doc*
   command. For example:

   : bin/search-cmd search_doc books "title:\"See spot run\""

** Querying via the Erlang Command Line

   To run a query from the Erlang shell, use =search:search(Query)=
   or =search:search(Index, Query)=. For example:

   : search:search("books", "author:joyce").

   This will display a list of Document ID values matching the
   query. To conduct a document search, use
   =search:search_doc(Query)= or =search:search_doc(Index,
   Query)=. For example:

   : search:search_doc("books", "author:joyce").

** Querying via the Solr Interface

   Riak Search supports a Solr-compatible interface for searching
   documents via HTTP. By default, the select endpoint is located at
   "http://hostname:8098/solr/select".

   Alternatively, the index can be included in the URL, for example
   "http://hostname:8098/solr/INDEX/select".

   The following parameters are supported:

   + *index=INDEX* - Specifies the default index name.

   + *q=QUERY* - Run the provided query.

   + *df=FIELDNAME* - Use the provided field as the default. Overrides
     the /default_field/ setting in the schema file.

   + *q.op=OPERATION* - Allowed settings are either "and" or
     "or". Overrides the /default_op/ setting in the schema
     file. Default is "or".

   + *start=N* - Specify the starting result of the query. Useful for
     paging. Default is 0.

   + *rows=N* - Specify the maximum number of results to return. Default is 10.

   + *sort=FIELDNAME* - Sort on the specified field name. Default is
     "none", which causes the results to be sorted in descending order
     by score.

   + *wt=FORMAT* - Choose the format of the output.  Options are "xml"
     and "json".  The default is "xml".

   To query data in the system with Curl:

   : curl "http://localhost:8098/solr/books/select?start=0&rows=10000&q=prog*"

** Faceted Queries via the Solr Interfae

   *NOTE:* Facet querying through the Solr interface is not yet supported.

   Faceted search allows you to generate keywords (plus counts) to
   display to a user to drill down into search results.

   Search accepts the following faceting parameters on the Solr
   interface:

   + *facet=BOOLEAN* - If BOOLEAN is set to "true" enable faceting. If
     set to "false", disable faceting. Default is "false".

   + *facet.field=FIELDNAME* - Tells Search to calculate and return
     count associated with unique terms in this fieldname. To specify
     multiple facet fields, include the /facet.field/ setting multiple
     times in the query parameters.

   + *facet.prefix=PREFIX* / *f.FIELD.facet.prefix=PREFIX* - Limit faceting to a
     subset of terms on a field.

   + *facet.sort=MODE* / *f.FIELD.facet.sort=MODE*- If MODE is set to
     "count", sort the facets counts by count. If set to "index", sort
     the facet counts lexicographically. Defaults to "count".

   + *facet.offset=N* / *f.FIELDNAME.facet.offset=N* - Set the offset
     at which to start listing facet entries. Used for paging.

   + *facet.limit=N* / *f.FIELDNAME.facet.limit=N* - Limit the number
     of facet entries to N. Used for paging.

   Use the longer syntax if multiple fields are defined.

   Note that when faceting on a field, only terms that are present in
   the result set are listed in the facet results (in other words, you will
   never see a facet count entry of zero.) Faceted fields are analyzed
   using the analyzer associated with the field.


** Query Scoring

   Documents are scored using roughly the same formulas described here:

   http://lucene.apache.org/java/3_0_2/api/core/org/apache/lucene/search/Similarity.html

   The key difference is in how Riak Search calculates the Inverse
   Document Frequency. The equations described on the /Similarity/
   page require knowledge of the total number of documents in a
   collection. Riak Search does not maintain this information for a
   collection, so instead uses the count of the total number of
   documents associated with each term in the query.

* Operations and Troubleshooting

  Riak Search has all of the same operational properties as
  Riak. Refer to the Riak wiki (see below) for more information on
  running Riak in a clustered environment.

  [[https://wiki.basho.com/display/RIAK/Home]]

** Default Ports

   By default, Search uses the following ports:

   + 8098 - Solr Interface

   + 8099 - Riak Handoff

   + 8087 - Protocol Buffers interface

   + 6095 - Analyzer Port

   Be sure to take the necessary security precautions to prevent
   exposing these ports to the outside world.
