#+SETUPFILE: "basho-search-doc-style.iorg"
#+TITLE: Riak Search Manual - DRAFT

* Introduction

  Basho Riak Search is a distributed index and full-text search engine
  built on top of (and complementary to) Basho Riak. Just like Riak,
  Search allows you to store your data in a distributed fashion
  designed for easy operations. You can add physical servers to gain
  capacity and performance, and if a node fails the cluster continues
  to function.

** Requirements

   The following components are required:

   + Erlang R13B04 (or later)
   + Java 1.6.x
   + Ant
   + gcc toolchain
   + BDB Java Edition 4.0.103 - (Installation instructions below.)

** Installation
   
*** Install BerkeleyDB
   
    Download the distribution file from:
    http://download.oracle.com/berkeley-db/je-4.0.103.tar.gz

    If you are compiling from source, copy lib/je-4.0.103.jar to your
    Riak Search directory, under apps/raptor/java_src/raptor/lib/.

    If you are using a pre-built Riak Search package, copy
    lib/je-4.0.103.jar to your Riak Search installation under
    lib/raptor/priv/.

*** Unzip and install Riak Search*

    Enter the riak\_search directory and run:

    : make
    : make rel
    
    Change to the rel/riaksearch directory

    : cd rel/riaksearch

*** Start Riak

    If you are running on a Mac, set JAVA\_HOME and increase your
    filehandle limit:
    
    : export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home
    : ulimit -n 1024
    
    From within the rel/riaksearch directory, start Riak:
    
    : bin/riaksearch console
    
    Or, use =bin/riaksearch start= to start riak\_search in the background.


** Major Components
   
   Riak Search is comprised of:

   + *Riak Core* -  Dynamo-inspired 
   + *Riak KV* - Distributed Key/Value store inspired by Amazon's Dynamo.
   + *Bitcask* -  Storage backend used by Riak KV.
   + *Riak Search* - Distributed index and full-text search engine.
   + *Raptor* - Storage backend used by Riak Search.
   + *Qilr* - Library for parsing queries into execution plans and
             documents into terms.
   + *Riak Solr* - Adds a subset of Solr HTTP interface capabilities to
                  Riak Search.

   Note that even though Riak KV is included in Riak Search, it
   *SHOULD NOT* be used to store application data. It is used intenally
   by Search to store document data, schema definitions, and other
   configuration data. Specifically, documents are stored in
   "INDEX\_doc" where INDEX is the name of the search schema. 

   *NOTE:* Don't yet know the bucket name for schema data. That work
   is currently in progress. Covered in bug #289.


** Replication

   Search data is replicated in a manner similar to Riak KV
   data: A search index has an =n_val= setting that determines how
   many copies of the data exist. Copies are written across different
   partitions located on different physical nodes.

   In contrast to Riak KV:

   + Search uses timestamps, rather than vector clocks, to resolve
     version conflicts. This leads to fewer guarantees about your data
     (as depending on wall-clock time can cause problems if the clock is
     wrong) but was a necessary tradeoff for performance reasons.

   + Search does not use quorum values when writing (indexing)
     data. The data is written in a fire and forget model. Search
     *does* use hinted-handoff to remain write-available when a node
     goes offline.

   + Search does not use quorum values when reading (querying)
     data. Only one copy of the data is read, and the partition is
     chosen based on what will create the most efficient query plan
     overall.

* Schema

  Riak Search was designed to work seamlessly with Riak. As a
  result, it retains many of the same properties as Riak, including
  a schema-free design. In other words, you can start adding data
  to a new index without having to explicitly define the index
  fields.

  That said, Search does provide the ability to define a schema,
  this is useful to set required fields and data types. This section
  describes how to define an index schema.
   
** Defining an Index Schema

   *NOTE:* Riak Search currently stores schema definition files in
   the lib/riak\_search/priv/ directory. This is covered in #289.

   Schema definitions are stored in Riak Search under a =schema=
   bucket. To create a new schema, write a new Riak object to the
   =schema= with a key equal to the schema name, and a value as the
   Erlang formatted schema definition file.

   : # Set an index schema.
   : bin/search-cmd set_schema Index SchemaFile
   : 
   : # View the schema for an Index.
   : bin/search-cmd view_schema Index

   There is currently no way to list all defined schemas.

   Below is an example schema file. This is an Erlang formatted
   term. Spacing does not matter, but it is important to matching
   opening and closing brackets or braces, and to include commas
   between all list items:

   : TODO - 
   : Put in example schema file. 
   : This is currently changing. Covered in #315.

** Changing the Schema File

   Changes to the Schema File *will not* affect previously indexed
   data. It is recommended that if you change field definitions,
   especially settings such as /inline/ or /type/, that you re-index
   your documents. The schema file can be changed using the same
   steps described above for setting the schema.

** Schema Properties
   
   A Schema definition consists of the following properties:
   
   + *version* - /Atom/, currently unused by Riak Search.

   + *default\_field* - /String/, the default field to use for
                      searching, if no other field is specified in
                      the query.

   + *default\_op* - /String/, either "and" or "or". Defaults to "or".

   + *analyzer\_factory* - Determines the Java class used to analyze the
        data. Defaults to
        *com.basho.search.analysis.DefaultAnalyzerFactory*, which is a
        StandardAnalyzer with a minimum length of 3 letters, lower
        cased, with stop words filtered using
        StopAnalyzer.ENGLISH\_STOP\_WORDS\_SET.

   + *fields* - See /Schema Field Properties/ section below.

** Schema Field Properties

   Each field consists of a field name string and the following
   properties:

   + *type* - /Atom/. Either 'string', 'boolean', 'integer', or
             # 'float'. Defaults to 'string'. See /Data Types/ section
             # below.

   + *required* - /Boolean/. Either 'true' or 'false'. Defaults to
                 # 'false'. 

   + *inline* - /Boolean/. Either 'true' or 'false'. Defaults to
               # 'false'. See /Inline Fields/ section below. 

** Inline Fields

   *NOTE:* These are called "facet" fields in the latest version of
   code. The name may or may not change to "inline" fields. This is
   covered in #314.

   Inline fields take their name from inline code. Just like inline
   code, the value of an inline field is repeated. The value of an
   inline field is transformed to a key/value property and stored
   along with each term of the document. For this reason, inline
   fields should be used carefully (and sparingly), as they
   substantially increase the amount of space used in an index.

   A typical use case might be to store the color of a
   product. Picture a product catalog with millions of products. A
   good portion of those products might be the color /red/, and so a
   query for =title:spatula AND color:red= would need to scan the
   entire /color:red/ section of the index to execute the query.

   If /color/ were made inline field, then /color/ would instead be
   stored as a property inline with every other value associated with
   the item. A query for =title:spatula AND color:red= would just
   read the /title:spatula/ section of the index, and then the
   =color:red= part of the query is applied as a second step in query
   execution, leading to less data transfer among query nodes and
   therefore faster queries.

   Note that both the field name and value are serialized, so if you
   are creating an inline field for color, you should consider using
   an abbreviated field name, such as /c/, and a small field value,
   such as /r/, rather than spelling out the entire word.


** Data Types
   
   Riak Search supports the following data types:

   * *string* - A field of type =string= is parsed using the
               # analyzer\_factory class set in the index's schema
               # file. 

   * *integer* - A field of type =integer= is always parsed
                # using the whitespace parser and padded to 10
                # digits. (In other words, the integer is converted
                # to a sortable string.) This allows for range
                # searches on integer fields and proper sorting.

   *NOTE:* The 10 digit padding is currently hard coded, but is due to
   change in issue #312.

* Indexing
  
  Indexing a document is the act of:
  
  1. Reading a document.
  2. Splitting the document into one or more fields.
  3. Splitting the fields into one or more terms.
  4. Normalizing the terms in each field.
  5. Writing the ={Field, Term, DocumentID}= postings to an index.

  There are numerous ways to index a document in Riak Search.

** Indexing via the Command Line

   The easiest way to index documents stored on the filesystem is to
   use the =search-cmd= command line tool:

   : bin/search-cmd index <INDEX> <PATH>

   Parameters:
   
   + *<INDEX>* - The name of an index.
                
   + *<PATH>* - Relative or absolute path to the files or directories
               to recursively index. Wildcards are permitted.
   
   For example:

   : bin/search-cmd index my_index files/to/index/*.txt

   The documents will be indexed into the default field defined by the
   Index's schema, using the base filename plus extension as the
   document ID.


** Deleting via the Command Line
   
   To remove previously indexed files from the command line, use the
   =search-cmd= command line tool:

   : bin/search-cmd delete <INDEX> <PATH>

   Parameters:
   
   + *<INDEX>* - The name of an index.
                
   + *<PATH>* - Relative or absolute path to the files or directories
               to recursively index. Wildcards are permitted.
   
   For example:

   : bin/search-cmd delete my_index files/to/index/*.txt

   Any documents matching the base filename plus extension of the
   files found will be removed from the index. The actual contents of
   the files are ignored during this operation.

** Indexing via the Erlang API

   The following Erlang functions will index documents stored on the
   filesystem:

   : search:index_dir(Path).
   : search:index_dir(Index, Path).

   + *Index* - The name of the index.
   
   + *Path* - Relative or absolute path to the files or directories to
             recursively index. Wildcards are 

   For example:

   : search:index_dir("my_index", "files/to/index/*.txt").

   The documents will be indexed into the default field defined by the
   Index's schema, using the base filename plus extension as the
   document ID.

   Alternatively, you can provide the fields of the document to index:

   : search:index_doc(Index, Fields)

   Parameters:

   + *Index* - The name of the index.

   + *Fields* - A Key/Value list of fields to index. One of these
               fields must be either the atom 'id' or the string
               "id".

   For example:

   : search:index_doc(Index, [{id, ID}, {title, "The Title"}, {content, "The Content"}])


** Deleting via the Erlang API

   The following Erlang functions will remove documents from the index:

   : search:delete_dir(Path).
   : search:delete_dir(Index, Path).

   + *Index* - The name of the index.
   
   + *Path* - Relative or absolute path to the files or directories to
             recursively index. Wildcards are 

   For example:

   : search:delete_dir("my_index", "files/to/index/*.txt").

   Any documents matching the base filename plus extension of the
   files found will be removed from the index. The actual contents of
   the files are ignored during this operation.

   Alternatively, you can provide the fields of the document to index:

   : search:delete_doc(Index, DocID)

   Parameters:

   + *Index* - The name of the index.

   + *DocID* - The document ID of the document to delete.


** Indexing via the Solr Interface

   Riak Search supports a Solr-compatible interface for indexing
   documents via HTTP. Documents must be formatted as simple Solr XML
   documents, for example:

   : <add>
   :   <doc>
   :     <field name="id">DocID</field>
   :     <field name="title">Zen and the Art of Motorcycle Maintenance</field>
   :     <field name="author">Robert Pirsig</field>
   :     ...
   :   </doc>
   :   ...
   : </add>

   Additionally, the Content-Type header must be set to 'text/xml'.

   Search currently requires that the field determining the document
   ID be named =id=, and does not support any additional attributes on
   the =add=, =doc=, or =field= elements. (In other words, things like
   =overwrite=, =commitWithin=, and =boost= are not yet supported.)

   The Solr interface does NOT support the =<commit />= nor =<optimize
   />= commands. All data is committed automatically in the following
   stages:

   + Incoming Solr XML document is parsed. If XML is invalid, an error
     is returned.

   + Documents fields are analyzed and broken into terms. If there are
     any problems, an error is returned.

   + Documents terms are indexed in parallel. Their availability in
     future queries is determined by the storage backend.

   By default, the update endpoint is located at
   "http://hostname:8098/solr/update?index=INDEX". 

   Alternatively, the index can be included in the URL, for example
   "http://hostname:8098/solr/INDEX/update".

   To add data to the system with Curl:

   : curl -X POST -H text/xml --data-binary @tests/books.xml http://localhost:8098/solr/books/update


** Deleting via the Solr Interface

   Documents can also be deleted through the Solr interface via two methods, either by Document ID or by Query.

   To delete documents by document ID, post the following XML to the update endpoint:

   : <delete>
   :   <id>docid1</id>
   :   <id>docid2</id>
   :   ...
   : </delete>

   To delete documents by Query, post the following XML to the update endpoint:

   : <delete>
   :   <query>QUERY1</query>
   :   <query>QUERY2</query>
   :   ...
   : </delete>

   Any documents that match the provided queries will be deleted.

** Indexing via Riak Post-Commit Hook

   *NOTE:* Search is not yet integrated with Riak. This is scheduled
   for a future release. 

   Riak (a distributed Key/Value store) can be configured to
   index new data automatically using Riak Search. Indexing is
   configured at a bucket level using a post-commit hook written in
   Erlang. (See [[https://wiki.basho.com/display/RIAK/Pre-+and+Post-Commit+Hooks][Pre and Post Commit Hooks]] for instructions on
   configuring Post-Commit hooks.)

   The following post-commit hook provides integration with Riak Search:
   
   : {"mod": "riak_search", "fun": "index"}

   Pending Questions:
   
   + How does Riak know /where/ Riak Search is located?
   + How does it know which index?
   + How does it break the value into fields.
   + Handle insert, update, and delete.
   + Handle case where the hook fails? Maybe do some preliminary work
     during pre-commit hook to make sure things parse correctly and
     can contact the index server.

* Querying

** Query Syntax

   Riak Search follows the same query syntax as Lucene, detailed
   here:  http://lucene.apache.org/java/2_4_0/queryparsersyntax.html

*** Terms and Phrases
    
    A query can be as simple as a single term (ie: "red") or a series
    of terms surrounded by quotes called a phrase ("See spot
    run"). The term (or phrase) is analyzed using the default
    analyzer for the index.

    The index schema contains a =default_operator= setting that
    determines whether a phrase is treated as an AND operation or an
    OR operation. By default, a phrase is treated as an OR
    operation. In other words, a document is returned if it matches
    any one of the terms in the phrase.

    *NOTE:* In the future, there will be different analyzers for each
    field. During a query, the terms will be parsed using the analyzer
    for that field. This is covered in #317.

*** Fields

    You can specify a field to search by putting it in front of the
    term or phrase to search. For example: 

    : color:red

    Or:

    : title:"See spot run"

    You can further specify an index by prefixing the field with the
    index name. For example:

    : products.color:red

    Or:

    : books.title:"See spot run"

*** Wildcard Searches

    Terms can include wildcards in the form of an asterisk (*) to
    allow prefix matching, or a question mark (?) to match a single
    character.

    Currently, the wildcard must come at the end of the term in both
    cases.

    For example:
    
    + "bus*" will match "busy", "business", "busted", etc.

    + "bus?" will match "busy", "bust", "busk", etc.

*** Fuzzy Searches
    
    Fuzzy searching allows you to find terms with similar
    spelling. To specify a fuzzy search, use the tilde operator on a
    single term with an optional fuzziness argument. (If no fuzziness
    argument is specified, then 0.5 is used by default.)

    For example:
    
    : bass~

    Is equivalent to:

    : bass~0.5

    And will match "bass" as well as "bask", "bats", "bars", etc. The
    fuzziness argument is a number between 0.0 and 1.0. Values close
    to 0.0 result in more fuzziness, values close to 1.0 result in
    less fuzziness.

*** Proximity Searches

    Proximity searching allows you to find terms that are within a
    certain number of words from each other. To specify a proximity
    seach, use the tilde argument on a phrase.

    For example:

    : "See spot run"~20

    Will find documents that have the words "see", "spot", and "run"
    all within the same block of 20 words.

*** Range Searches
    
    Range searches allow you to find documents with terms in between
    a specific range. Ranges are calculated lexicographically.  Use
    square brackets to specify an inclusive range, and curly braces
    to specify an exclusive range.

    The following example will return documents with words containing
    "red" and "rum", plus any words in between.

    : "[red TO rum]" 
    
    The following example will return documents with words in between
    "red" and "rum":

    : "{red TO rum}"

*** Boosting a Term

    A term (or phrase) can have its score boosted using the caret
    operator along with an integer boost factor.

    In the following example, documents with the term "red" will have
    their score boosted:

    : red^5 OR blue

*** Boolean Operators - AND, OR, NOT

    Queries can use the boolean operators AND, OR, and NOT. The
    boolean operators must be capitalized.

    The following example return documents containing the words "red"
    and "blue" but not "yellow".

    : red AND blue AND NOT yellow

    The required (+) operator can be used in place of "AND", and the
    prohibited (-) operator can be used in place of "AND NOT". For
    example, the query above can be rewritten as:

    : +red +blue -yellow

*** Grouping

    Clauses in a query can be grouped using parentheses. The following
    query returns documents that contain the terms "red" or "blue",
    but not "yellow":

    : (red OR blue) AND NOT yellow

** Querying via the Search Shell

   The Search Shell is the easiest way to run interactive queries
   against Search. To start the shell, run:

   : bin/search-cmd shell [INDEX]

   This launches an interactive console into which you can type search
   commands. For help, type =h()=. 


** Querying via the Command Line

   To run a single query from the command line, use:

   : bin/search-cmd search [INDEX] QUERY

   For example:

   : bin/search-cmd search "books" "title:\"See spot run\""

   This will display a list of Document ID values matching the
   query. To conduct a document search, use the *search\_doc*
   command. For example:

   : bin/search-cmd search_doc "books" "title:\"See spot run\""

** Querying via the Erlang Command Line

   To run a query from the Erlang shell, use =search:search(Query)=
   or =search:search(Index, Query)=. For example:
   
   : search:search("books", "author:joyce").
   
   This will display a list of Document ID values matching the
   query. To conduct a document search, use
   =search:search_doc(Query)= or =search:search_doc(Index,
   Query)=. For example:
   
   : search:search_doc("books", "author:joyce").

** Querying via the Solr Interface

   Riak Search supports a Solr-compatible interface for searching
   documents via HTTP. By default, the select endpoint is located at
   "http://hostname:8098/solr/select".

   Alternatively, the index can be included in the URL, for example
   "http://hostname:8098/solr/INDEX/select".

   The following parameters are supported:

   + *index=INDEX* - Specifies the default index name.

   + *q=QUERY* - Run the provided query.

   + *df=FIELDNAME* - Use the provided field as the default. Overrides
     the /default_field/ setting in the schema file.

   + *q.op=OPERATION* - Allowed settings are either "and" or
     "or". Overrides the /default_op/ setting in the schema
     file. Default is "or".

   + *start=N* - Specify the starting result of the query. Useful for
     paging. Default is 0.

   + *rows=N* - Specify the maximum number of results to return. Default is 10.

   + *sort=FIELDNAME* - Sort on the specified field name. Default is
     "none", which causes the results to be sorted in descending order
     by score.

   + *wt=FORMAT* - Choose the format of the output.  Options are "xml"
     and "json".  The default is "xml".

   To query data in the system with Curl:

   : curl "http://localhost:8098/solr/books/select?start=0&rows=10000&q=prog*"

** Faceted Queries via the Solr Interfae
   
   *NOTE:* The facet features listed below do not yet exist. These are
   covered in #326.

   Faceted search allows you to generate keywords (plus counts) to
   display to a user to drill down into search results.

   Search accepts the following faceting parameters on the Solr
   interface:

   + *facet=BOOLEAN* - If BOOLEAN is set to "true" enable faceting. If
     set to "false", disable faceting. Default is "false".

   + *facet.field=FIELDNAME* - Tells Search to calculate and return
     count associated with unique terms in this fieldname. To specify
     multiple facet fields, include the /facet.field/ setting multiple
     times in the query parameters.

   + *facet.prefix=PREFIX* / *f.FIELD.facet.prefix=PREFIX* - Limit faceting to a
     subset of terms on a field.

   + *facet.sort=MODE* / *f.FIELD.facet.sort=MODE*- If MODE is set to
     "count", sort the facets counts by count. If set to "index", sort
     the facet counts lexicographically. Defaults to "count".

   + *facet.offset=N* / *f.FIELDNAME.facet.offset=N* - Set the offset
     at which to start listing facet entries. Used for paging.

   + *facet.limit=N* / *f.FIELDNAME.facet.limit=N* - Limit the number
     of facet entries to N. Used for paging.

   Use the longer syntax if multiple fields are defined.

   Note that when faceting on a field, only terms that are present in
   the result set are listed in the facet results (in other words, you will
   never see a facet count entry of zero.) Faceted fields are analyzed
   using the analyzer associated with the field.


** Querying via Riak Map/Reduce
   
   /This functionality is planned for a future release./

** Query Scoring
   
   Documents are scored using roughly the same formulas described here: 

   http://lucene.apache.org/java/3_0_2/api/core/org/apache/lucene/search/Similarity.html

   The key difference is in how Riak Search calculates the Inverse
   Document Frequency. The equations described on the /Similarity/
   page require knowledge of the total number of documents in a
   collection. Riak Search does not maintain this information for a
   collection, so instead uses the count of the total number of
   documents associated with each term in the query.

* Operations and Troubleshooting

  Riak Search has all of the same operational properties as
  Riak. Refer to the Riak wiki
  [[https://wiki.basho.com/display/RIAK/Home]] for more information on
  running Riak in a clustered environment.

** Default Ports

   By default, Search uses the following ports:

   + 8098 - Solr Interface

   + 8099 - Riak Handoff

   + 8087 - Protocol Buffers interface

   + 5099 - Raptor Backend

   + 6095 - Analyzer Port

   Be sure to take the necessary security precautions to prevent
   exposing these ports to the outside world.

** The Raptor Backend

*** How do I change Raptor Logging?

    1. You can edit log4j.properties to change the logging characteristics

    2. From the riak search console you can significantly decrease
       logging output (though not turn it off entirely) by using the
       command:
        
       : raptor_index_backend:poke("toggle_debug").

*** How do I delete all data from Raptor?
    
    If you want to delete the data stored in Raptor, delete the
    "./data/raptor" directory.

*** Notes

    1. MMap and Cache memory use for both BDB and Lucene are currently hard 
       coded while a good configuration system is being worked on.  In the 
       meantime it is recommended you run Raptor on server with a MINIMUM 
       of 2G ram.

    2. You may use Raptor on 32-bit systems; change the command line
       option in start.sh from "-d64" to "-d32"

    3. 32-bit operation is not available on Mac OS X systems (Java 1.6
       has no 32 bit mode on OS X)

    4. If you wish to keep the BDB native library in somewhere other
       than it's installed location, change the following command line
       option in start.sh to reflect the new location:

       : -Djava.library.path=/usr/local/lib:java.library.path=/usr/local/lib:/usr/local/BerkeleyDB.4.8/lib

    5. Raptor has only been tested on Mac OS X and Linux.

    6. If you see this, upgrade to Java 1.6:

       : [javac] bad class file: ...snip.../db.jar(com/sleepycat/db/Environment.class)
       : [javac] class file has wrong version 50.0, should be 49.0
       : [javac] Please remove or make sure it appears in the correct subdirectory of the classpath.
       : [javac]     private Environment env;
       : [javac]             ^
       : [javac] 1 error
         
